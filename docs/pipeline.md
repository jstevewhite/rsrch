# research pipeline

Realized in python, configurable with a configuration file or .env file. 

API_KEY=<api key>
API_ENDPOINT=<api_endpoint>
DEFAULT_MODEL=<model name>

## Accept general query:

"What is the latest research on tirzepatide?"
"How do you initialize litellm and use it in a python program?"
"What's the latest news on the Israel/Gaza conflict?"

## Identify intent

LLM is asked to identify the intent of the query - categories like "INFORMATIONAL", "COMPARATIVE", "NEWS", "CODE" and more.

Returns the intent.

INTENT_MODEL=<model name>

## Planner

LLM plans the layout of the report based on the query and the intent, and designs queries to fulfill the design.

PLANNTER_MODEL=<model name>

## Researcher

Searches for the queries generated by the Planner

Gathers URLS + SERP, re-ranks (top-k of 25%, for example)

## Scraper

scrapes the selected URLs, chunks them appropriately, and embeds them in a vector database (sqlite + vss); includes metadata like 'date retrieved', 'url', 'next chunk', 'previous chunk'. 

## Content-Aware Map-reduce Summarizer

creates summaries with metadata (original URL) and CITABLE quotes relevant to the topic. 

MRS_MODEL=<model name>

## Context assembler

COnstructs a context package with relevant information; the eventual goal is to submit to a frontier model for report construction. 

CONTEXT_MODEL=<model name>

## Reflection stage

Consideres teh query and the plan and asks "Do we have what we need to answer all the questions in the plan?" If not, it can create new searches.

When the reflection stage is satisfied:

REFLECTION_MODEL=<model name>

## Report construction

Context package is sent to the LLM and the LLM constructs a report. 

REPORT_MODEL=<model name>
